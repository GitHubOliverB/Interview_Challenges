{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear_Regression_Modelling\n",
    "\n",
    "Consider two data series, $X = \\left(x_{1}, x_{2}, ..., x_{n}\\right)$ and $Y = \\left(y_{1}, y_{2}, ..., y_{n}\\right)$, both with mean zero. We use linear regression (ordinary least squares) to regress $Y$ against $X$ (without ﬁtting any intercept), as in $Y = aX + \\epsilon$ where $\\epsilon$ denotes a series of error terms.\n",
    "\n",
    "Problems:\n",
    "\n",
    "1. Calculate the value of the regression coefﬁcient $a$. If possible, express it in terms of the standard deviations $\\sigma_{X}$ and $\\sigma_{Y}$ and the correlation coefficient $\\rho_{XY}$ between the two data series. You will need to show a complete derivation to score full marks.  \n",
    "\n",
    "2. We scale up both data series by constant factors $s$ and $t$, i.e. $X' = sX$ and $Y' = tY$ , and regress $Y'$ against $X'$ as in $Y' = a'X' + \\epsilon$. How does the new regression coefﬁcient $a'$ relate to the original coefﬁcient $a$? And what about the new correlation $\\rho_{X'Y'}$ vs. the original correlation $\\rho_{XY}$ ? Note that the new $\\epsilon$ is not necessarily the same as the original one, it merely denotes another series of error terms.  \n",
    "\n",
    "3. We now do the ‘inverse’ regression of $X$ against $Y$ , resulting in $X = bY + \\epsilon$. How is the slope $b$ of the ‘inverse’ regression related to the slope $a$ of the original regression?   \n",
    "\n",
    "4. Suppose that $\\rho_{XY} = 0.01$. Is the resulting value of $a$ statistically signiﬁcantly different from $0$ at the $95\\,\\%$ level if:   \n",
    "    i. $n = 10^{2}$  \n",
    "    ii. $n = 10^{3}$  \n",
    "    iii. $n = 10^{4}$  \n",
    "\n",
    "In order to make life a bit easier, I'm going to assume that $\\epsilon \\sim \\mathcal{N}\\left(0,\\sigma_{\\epsilon}^{2}\\right)$, $\\textit{Cov}\\left[\\epsilon_{i}, \\epsilon_{j}\\right] = \\sigma_{\\epsilon}^{2} \\cdot \\delta_{ij}$ and $\\textit{Var}\\left[\\epsilon_{i}\\right] = \\sigma^{2}_{\\epsilon}\\,\\forall i\\in\\{1\\dots,n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:\n",
    "\n",
    "The idea of linear regression is to use a function $f^{*}\\left(X\\right)$ that is linear in a set of parameters $a_{i}\\in A$ to predict $Y$ as close as possible. The function $f^{*}\\left(X\\right)$ is derived by chosing the parameters $a_{i}\\in A$ for a function $f\\left(A, X\\right)$ so that this is achieved.\n",
    "\n",
    "In our case we work with $f\\left(a, X\\right) = a\\cdot X$ so that $Y = a\\cdot X + \\epsilon$. \n",
    "\n",
    "When we use ordinary least square to regress $Y$ to $X$, what we want to do is to minimize is the squared difference between our observed values $Y$ and the prediction from our function $f\\left(a, X\\right)$. We denote that as our loss function $L$, given by:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\sum_{i=1}^{n} \\left(y_{i}-a\\cdot x_{i}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "By minimizing $L$, we can find the set of parameters for which $f\\left(A, X\\right)$ becomes $f^{*}\\left(X\\right)$, so let's do that now.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial a} &=& \\frac{\\partial}{\\partial a}\\sum_{i=1}^{n} \\left(y_{i}-a\\cdot x_{i}\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}\\frac{\\partial}{\\partial a} \\left(y_{i}-a\\cdot x_{i}\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}2\\cdot\\left(y_{i}-a\\cdot x_{i}\\right)\\cdot\\left(-x_{i}\\right) \\\\\n",
    "&=& -2\\cdot\\sum_{i=1}^{n}\\left(y_{i}\\cdot x_{i}-a\\cdot x_{i}^{2}\\right) \\\\\n",
    "&=& -2\\cdot\\left(\\sum_{i=1}^{n}y_{i}\\cdot x_{i}\\right) + 2\\cdot\\left(\\sum_{i=1}^{n} a\\cdot x_{i}^{2}\\right) \\\\\n",
    "&=& -2\\cdot\\left(\\sum_{i=1}^{n}y_{i}\\cdot x_{i}\\right) + 2a\\cdot\\left(\\sum_{i=1}^{n} x_{i}^{2}\\right) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "From setting $\\frac{\\partial L}{\\partial a}=0$, it follows that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a\\cdot\\sum_{i=1}^{n} x_{i}^{2} &=& \\sum_{i=1}^{n}y_{i}\\cdot x_{i}  \\\\\n",
    "a &=& \\frac{\\sum_{i=1}^{n}y_{i}\\cdot x_{i}}{\\sum_{i=1}^{n} x_{i}^{2}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now we have found an expression for $a$ in terms of sums over $x_{i}$ and $y_{i}$. However we want to express it in terms of the standard deviations $\\sigma_{X}$ and $\\sigma_{Y}$ and the correlation $\\rho_{XY}$. So let's work out those and see if we can substitute them in our result.\n",
    "\n",
    "The standard deviations $\\sigma_{k}$ for a sample with $k = X, Y$ and $\\bar{k}=\\frac{1}{n}\\sum_{i=1}^{n}k_i$ as the mean value of $k$ is given by:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{k}^{2} = \\frac{\\sum_{i=1}^{n}\\left(k_{i}-\\bar{k}\\right)^{2}}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "Now since the mean values of $X$ and $Y$ are both zero, we can simplify the standard deviations by substituting $\\bar{k}=0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{k}^{2} = \\frac{\\sum_{i=1}^{n}k_{i}^{2}}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "The correlation coefficient $\\rho_{XY}$ for a sample is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{XY} = \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "Using our mean values of zero, we simply get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{XY} = \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}\\sqrt{\\sum_{i=1}^{n}y_{i}^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "We can expand our equation for $\\rho_{XY}$ by multiplying with $\\frac{1}{\\frac{n-1}{n-1}}$ and get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\rho_{XY} &=& \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sqrt{\\frac{\\sum_{i=1}^{n}x_{i}^{2}}{n-1}}\\sqrt{\\frac{\\sum_{i=1}^{n}y_{i}^{2}}{n-1}}}\\cdot \\frac{1}{n-1} \\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\sigma_{Y}}\\cdot \\frac{1}{n-1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We recognize that the sums in $sigma_{X}$ and in $\\rho_{XY}$ appear in $a$, so let's expand $a$ so that we substitute in both:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a &=& \\frac{\\sum_{i=1}^{n}y_{i}\\cdot x_{i}}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\sum_{i=1}^{n}y_{i}\\cdot x_{i} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}\\sigma_{Y}}\\frac{n-1}{n-1} \\cdot \\frac{1}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\left(\\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\sigma_{Y}}\\cdot \\frac{1}{n-1}\\right) \\cdot \\frac{\\sigma_{X}\\sigma_{Y}\\cdot\\left(n-1\\right)}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\frac{\\sum_{i=1}^{n} x_{i}^{2}}{n-1}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}^{2}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{Y}}{\\sigma_{X}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "So we find that $a$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\rho_{XY} \\cdot \\frac{\\sigma_{Y}}{\\sigma_{X}}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore our function $f^{*}\\left(X\\right)$ is given by:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f^{*}\\left(X\\right) = \\frac{\\rho_{XY}\\cdot\\sigma_{Y}}{\\sigma_{X}} \\cdot X\n",
    "\\end{equation}\n",
    "\n",
    "I want to test this, so first let's grab a useful expression for $\\rho_{XY}$. Using the covariance $\\textit{cov}\\left(X,Y\\right)$ of $X$ and $Y$ given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textit{cov}\\left(X,Y\\right) = \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(y_{i}-\\bar{y}\\right)}{n-1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We can rewrite the correlation coefficient as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\rho_{XY} = \\frac{\\textit{cov}\\left(X,Y\\right)}{\\sigma_{X}\\sigma_{Y}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Thus $a$ can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\frac{\\textit{cov}\\left(X,Y\\right)}{\\sigma_{X}^{2}}\n",
    "= \\frac{\\textit{cov}\\left(X,Y\\right)}{\\textit{var}\\left(X\\right)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Y: 0.00\n",
      "Mean of X: 0.05\n",
      "Due to statistical fluctuations the mean value of X is not exactly 0.\n",
      "\n",
      "Our guess for the slope a is: 0.990\n",
      "\n",
      "Running the linear regression with a forced intercept of 0, we get:\n",
      "\n",
      "Slope a: 0.990\n",
      "Intercept: 0.0\n",
      "\n",
      " The difference between our guess and the found slope is: 0.00006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mu_X = range(-10,11,1)\n",
    "sigma_X = 0.5\n",
    "measurements = 10\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for mu in mu_X:\n",
    "    Y.extend([mu]*measurements)\n",
    "    X.extend(np.random.normal(mu, sigma_X, measurements))\n",
    "\n",
    "print(\"Mean of Y: {0:.2f}\".format(np.mean(Y)))\n",
    "print(\"Mean of X: {0:.2f}\".format(np.mean(X)))\n",
    "if np.mean(X) != 0:\n",
    "    print(\"Due to statistical fluctuations the mean value of X is not exactly 0.\\n\")\n",
    "    \n",
    "cov_XY_matrix = np.cov(X,Y)\n",
    "cov_XY = cov_XY_matrix[0,1]\n",
    "var_X = cov_XY_matrix[0,0]\n",
    "var_Y = cov_XY_matrix[1,1]\n",
    "\n",
    "a = cov_XY / var_X\n",
    "\n",
    "print(\"Our guess for the slope a is: {0:.3f}\\n\".format(a))\n",
    "\n",
    "# Do linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_reg = np.array(X).reshape((-1, 1))\n",
    "\n",
    "reg = LinearRegression(fit_intercept=0).fit(X_reg, Y)\n",
    "\n",
    "print(\"Running the linear regression with a forced intercept of 0, we get:\\n\")\n",
    "print(\"Slope a: {0:.3f}\".format(reg.coef_[0]))\n",
    "print(\"Intercept: {}\".format(reg.intercept_))\n",
    "\n",
    "print(\"\\n The difference between our guess and the found slope is: {0:.5f}\".format(abs(reg.coef_[0]-a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see our result seems to be pretty good. Since $\\bar{x}$ is not exactly $0$, we have a small difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:\n",
    "\n",
    "So let's see what scaling both $X$ and $Y$ does to everything that we have calculated so far. In order to do that we simply have to follow:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "X &\\longrightarrow& X' = s\\cdot X \\\\\n",
    "Y &\\longrightarrow& Y' = t\\cdot Y\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We can start by the mean and find that is scaled by the specific factor, but since the old mean was zero, it remains unchanged:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\bar{x}' &=& \\frac{1}{n}\\cdot\\sum_{i=1}^{n} x_{i}' &=& \\frac{1}{n}\\cdot\\sum_{i=1}^{n}s\\cdot x_{i} &=& s\\bar{x} &=& 0\\\\\n",
    "\\bar{y}' &=&  \\frac{1}{n}\\cdot\\sum_{i=1}^{n} y_{i}' &=& \\frac{1}{n}\\cdot\\sum_{i=1}^{n}t\\cdot y_{i} &=& t\\bar{y} &=& 0\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Knowing $\\bar{x}'$ and $\\bar{y}'$ we can work out $\\sigma_{X'}$ and $\\sigma_{Y'}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sigma_{X'}^{2} &=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}'-\\bar{x}'\\right)^{2}}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}s^{2}x_{i}^{2}}{n-1} \n",
    "= s^{2}\\cdot\\frac{\\sum_{i=1}^{n}x_{i}^{2}}{n-1} \n",
    "= s^{2}\\sigma_{X}^{2} \\\\\n",
    "\\\\\n",
    "\\sigma_{Y'}^{2} &=& \\frac{\\sum_{i=1}^{n}\\left(y_{i}'-\\bar{y}'\\right)^{2}}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}t^{2}y_{i}^{2}}{n-1} \n",
    "= t^{2}\\cdot\\frac{\\sum_{i=1}^{n}y_{i}^{2}}{n-1} \n",
    "= t^{2}\\sigma_{Y}^{2} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "And from there we can look at $\\rho_{X'Y'}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{X'Y'} = \\frac{\\sum_{i=1}^{n}x_{i}'\\cdot y_{i}'}{\\sigma_{X'}\\sigma_{Y'}}\\cdot \\frac{1}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}sx_{i} \\cdot t y_{i}}{s\\sigma_{X}\\cdot t\\sigma_{Y}}\\cdot \\frac{1}{n-1} \n",
    "= \\frac{st\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{st\\cdot\\sigma_{X}\\cdot\\sigma_{Y}}\\cdot \\frac{1}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\cdot\\sigma_{Y}}\\cdot \\frac{1}{n-1} \n",
    "= \\rho_{XY}\n",
    "\\end{equation}\n",
    "\n",
    "As we can see $\\rho_{X'Y'}$ is unchanged just like the mean values of $X$ and $Y$. Now we have everything we need to calculate the slope $a'$:\n",
    "\n",
    "\\begin{equation}\n",
    "a' = \\frac{\\rho_{X'Y'}\\cdot\\sigma_{Y'}}{\\sigma_{X'}}\n",
    "= \\frac{\\rho_{XY}\\cdot t\\sigma_{Y}}{s\\sigma_{X}}\n",
    "= \\frac{t}{s}\\cdot \\frac{\\rho_{XY}\\cdot\\sigma_{Y}}{\\sigma_{X}}\n",
    "= \\frac{t}{s}\\cdot a\n",
    "\\end{equation}\n",
    "\n",
    "So finally, we can see that $a'$ is just $a$ scaled by the ratio $\\frac{t}{s}$. That makes perfect scence, since we also could have started with $Y' = a'X' + \\epsilon$ and work out $a'$ from there in the following fashion:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Y' = a'X' + \\epsilon \\\\\n",
    "\\\\\n",
    "tY = a'sX+ \\epsilon \\\\\n",
    "\\\\\n",
    "Y = \\frac{s}{t}\\cdot a' \\cdot X + \\frac{\\epsilon}{t}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "And recognize that :\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\frac{s}{t}\\cdot a'\n",
    "\\end{equation}\n",
    "\n",
    "Furhtermore, we can also think through a few cases. If we left $Y$ unscaled, so $t=1$ and we scale up $X$ by a factor $s>1$, then $a'$ must be smaller than $a$ to compensate the upscaling of $X$ for $Y$ to remain unchanged. On the other hand, if we scale up $Y$ by a factor $t>1$ and we left $X$ unchanged, then $a'$ must be bigger than $a$ to compensate the upscaling of $Y$. So $a'$ must depend on both scaling factors.\n",
    "\n",
    "Now let's check that again with the same piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Y_scale: -0.00\n",
      "Mean of X_scale: 0.35\n",
      "Due to statistical fluctuations the mean value of X_scale is not exactly 0.\n",
      "\n",
      "Our guess for the slope a_scale is: 0.027\n",
      "\n",
      "Running the linear regression with a forced intercept of 0, we get:\n",
      "\n",
      "Slope a_scale: 0.027\n",
      "Intercept_scale: 0.0\n",
      "\n",
      " The difference between our guess and the found slope is: 0.00000\n"
     ]
    }
   ],
   "source": [
    "s = 7.4\n",
    "t = 1/5\n",
    "\n",
    "X_scale = [s*x for x in X]\n",
    "Y_scale = [t*y for y in Y]\n",
    "\n",
    "print(\"Mean of Y_scale: {0:.2f}\".format(np.mean(Y_scale)))\n",
    "print(\"Mean of X_scale: {0:.2f}\".format(np.mean(X_scale)))\n",
    "if np.mean(X_scale) != 0:\n",
    "    print(\"Due to statistical fluctuations the mean value of X_scale is not exactly 0.\\n\")\n",
    "    \n",
    "a_scale = a * t/s\n",
    "\n",
    "print(\"Our guess for the slope a_scale is: {0:.3f}\\n\".format(a_scale))\n",
    "\n",
    "# Do linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_scale_reg = np.array(X_scale).reshape((-1, 1))\n",
    "\n",
    "reg_scale = LinearRegression(fit_intercept=0).fit(X_scale_reg, Y_scale)\n",
    "\n",
    "print(\"Running the linear regression with a forced intercept of 0, we get:\\n\")\n",
    "print(\"Slope a_scale: {0:.3f}\".format(reg_scale.coef_[0]))\n",
    "print(\"Intercept_scale: {}\".format(reg_scale.intercept_))\n",
    "\n",
    "print(\"\\n The difference between our guess and the found slope is: {0:.5f}\".format(abs(reg_scale.coef_[0]-a_scale)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:\n",
    "\n",
    "Since we didn't change the structure of our problem, we can get the result by taking the old result and act with the following transformation on it:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Y &\\longrightarrow& X \\\\\n",
    "X &\\longrightarrow& Y \\\\\n",
    "a &\\longrightarrow& b \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Hence:\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\rho_{YX}\\cdot\\sigma_{X}}{\\sigma_{Y}} \n",
    "= \\frac{\\rho_{XY}\\sigma_{X}}{\\sigma_{Y}}\n",
    "= \\frac{\\rho_{XY}\\sigma_{X}}{\\sigma_{Y}}\\cdot\\frac{\\sigma_{X}}{\\sigma_{X}}\\cdot\\frac{\\sigma_{Y}}{\\sigma_{Y}}\n",
    "= \\frac{\\rho_{XY}\\sigma_{Y}}{\\sigma_{X}}\\cdot\\frac{\\sigma_{X}^{2}}{\\sigma_{Y}^{2}}\n",
    "= a\\cdot \\frac{\\sigma_{X}^{2}}{\\sigma_{Y}^{2}}\n",
    "= a\\cdot \\frac{\\textit{var}\\left(X\\right)}{\\textit{var}\\left(Y\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "So let us test that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our guess for the slope b is: 1.003\n",
      "\n",
      "Running the linear regression with a forced intercept of 0, we get:\n",
      "\n",
      "Slope b: 1.003\n",
      "Intercept inverse: 0.0\n",
      "\n",
      " The difference between our guess and the found slope is: 0.00000\n"
     ]
    }
   ],
   "source": [
    "b = a * var_X / var_Y\n",
    "\n",
    "print(\"Our guess for the slope b is: {0:.3f}\\n\".format(b))\n",
    "\n",
    "# Do linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "Y_reg = np.array(Y).reshape((-1, 1))\n",
    "\n",
    "reg_inverse = LinearRegression(fit_intercept=0).fit(Y_reg, X)\n",
    "\n",
    "print(\"Running the linear regression with a forced intercept of 0, we get:\\n\")\n",
    "print(\"Slope b: {0:.3f}\".format(reg_inverse.coef_[0]))\n",
    "print(\"Intercept inverse: {}\".format(reg_inverse.intercept_))\n",
    "\n",
    "print(\"\\n The difference between our guess and the found slope is: {0:.5f}\".format(abs(reg_inverse.coef_[0]-b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, let's move on to the final problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4:\n",
    "\n",
    "Now we want to check two hypotheses, the null-hypothesis $H_{0}$ stating that the slope $a$ is zero and the alternative hypothesis $H_{a}$ stating that the slope $a$ is not zero.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "H_{0}: a = 0 \\\\\n",
    "H_{a}: a \\neq 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now I normally would calculate the test statistic $t$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\hat{a}-a_{0}}{\\textit{SE}\\left[\\hat{a}\\right]}\n",
    "\\end{equation}\n",
    "\n",
    "With $\\hat{a}$ as our estimated value for the slope $a$, $a_0 = 0$ the value of for the slope a in the case of our null-hypothesis and $\\textit{SE}\\left[\\hat{a}\\right]$ as the standard error for $\\hat{a}$. Once we have the t-statistic, we can calculate the values for a given $n$ and compare that to a table for the $95\\,\\%$ significant level and see if our result is significant.\n",
    "\n",
    "Now, I don't know a simple formula to calculate that by heart, so let's derive it (it will be long).\n",
    "\n",
    "Let's start by finding the expected value for $\\hat{a}$. For that we will our result for $\\hat{a}$ from problem 1:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{a} = \\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "Given that both $\\sum_{i=1}^{n}\\left(k_{i}-\\bar{k}\\right) = 0$ for $k = x, y$, we can expand both sums and get rid of the term that is multiplied by $\\bar{k}$, since we can just pull that infront of the sum. Hence:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{a} = \\frac{\\sum_{i=1}^{n}y_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "Furhtermore we will need a neat little property of the expected value called the linearity of expectations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathit{E}\\left[aX+bY\\right] = \\frac{1}{n}\\cdot\\sum_{i=1}^{n}\\left(aX+bY\\right)\n",
    "= \\frac{1}{n}\\cdot\\left(\\sum_{i=1}^{n}aX\\right) + \\frac{1}{n}\\cdot\\left(\\sum_{i=1}^{n}bY\\right)\n",
    "= \\frac{a}{n}\\cdot\\left(\\sum_{i=1}^{n}X\\right) + \\frac{b}{n}\\cdot\\left(\\sum_{i=1}^{n}Y\\right)\n",
    "= a\\cdot \\mathit{E}\\left[X\\right] + b \\cdot \\mathit{E}\\left[Y\\right]\n",
    "\\end{equation}\n",
    "\n",
    "On top of that we will assume $X$ to be known fixed values so that $\\mathit{E}\\left[x_{i}\\right] = x_{i}$, $\\forall i \\in \\{1, \\dots, n\\}$ and that the error terms are normally distributed around the mean value of $0$ so that $\\mathit{E}\\left[\\epsilon_{i}\\right] = 0$, $\\forall i \\in \\{1, \\dots, n\\}$.\n",
    "\n",
    "Now let's begin to derive the expected value of $\\hat{a}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathit{E}\\left[\\hat{a}\\right] &=& \\mathit{E}\\left[\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\right]\\\\\n",
    "&=& \\frac{1}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\cdot\\mathit{E}\\left[\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]\\\\\n",
    "&=& \\frac{\\mathit{E}\\left[\\sum_{i=1}^{n}y_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\mathit{E}\\left[y_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\mathit{E}\\left[\\left(a\\cdot x_{i} + \\epsilon_{i}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\mathit{E}\\left[a\\cdot x_{i} + \\epsilon_{i}\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(a\\cdot x_{i}+\\mathit{E}\\left[\\epsilon_{i}\\right]\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot a\\cdot x_{i}}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& a \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Great. As we can see we have an unbiased estimater for the slope $a$.\n",
    "\n",
    "Now let's have a look at the variance of $\\hat{a}$. But again let's quickly introduce some properties that we will need. Also the assumption about our fixed known values of $X$ still stands. We will start of with the definition of the variance and show that there is no linearity for the variance and that the variance of a constant vanishes. Given the definition of the variance\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textit{Var}\\left[X\\right] &=& \\mathit{E}\\left[\\left(X - \\mathit{E}\\left[X\\right]\\right)^{2}\\right] \\\\\n",
    "&=& \\mathit{E}\\left[X^{2} + \\left(\\mathit{E}\\left[X\\right]\\right)^{2} - 2X\\mathit{E}\\left[X\\right]\\right] \\\\\n",
    "&=& \\mathit{E}\\left[X^{2}\\right] + \\left(\\mathit{E}\\left[X\\right]\\right)^{2} - 2\\left(\\mathit{E}\\left[X\\right]\\mathit{E}\\left[X\\right]\\right) \\\\\n",
    "&=& \\mathit{E}\\left[X^{2}\\right] - \\left(\\mathit{E}\\left[X\\right]\\right)^{2}\\, ,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "we can work out that for a constant $a$ and $c$ and a random variable $X$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textit{Var}\\left[aX + c\\right] &=& \\mathit{E}\\left[\\left(aX + c - \\mathit{E}\\left[aX + c\\right]\\right)^{2}\\right] \\\\\n",
    "&=& \\mathit{E}\\left[\\left(aX\\right)^{2} + c^{2} + \\left(\\mathit{E}\\left[aX + c\\right]\\right)^{2} + 2aX\\cdot c - 2aX\\cdot\\mathit{E}\\left[aX + c\\right] - 2c\\cdot\\mathit{E}\\left[aX + c\\right]\\right] \\\\\n",
    "&=& \\mathit{E}\\left[\\left(aX\\right)^{2}\\right] + \\mathit{E}\\left[c^{2}\\right] + \\left(\\mathit{E}\\left[aX + c\\right]\\right)^{2} + \\mathit{E}\\left[2aX\\cdot c\\right] - \\mathit{E}\\left[2aX\\cdot\\mathit{E}\\left[aX + c\\right]\\right] - \\mathit{E}\\left[2c\\cdot\\mathit{E}\\left[aX + c\\right]\\right] \\\\\n",
    "&=& a^{2}\\mathit{E}\\left[X^{2}\\right] + c^{2} + \\left(a\\mathit{E}\\left[X\\right] + c\\right)^{2} + 2ac\\cdot\\mathit{E}\\left[X\\right] - 2a\\mathit{E}\\left[X\\right]\\cdot\\mathit{E}\\left[aX + c\\right] - 2c\\cdot\\mathit{E}\\left[aX + c\\right] \\\\\n",
    "&=& a^{2}\\mathit{E}\\left[X^{2}\\right] + c^{2} + a^{2}\\left(\\mathit{E}\\left[X\\right]\\right)^{2} + c^{2} + 2ac\\mathit{E}\\left[X\\right] + 2ac\\cdot\\mathit{E}\\left[X\\right] - 2a\\mathit{E}\\left[X\\right]\\cdot\\left(a\\mathit{E}\\left[X\\right] + c\\right) - 2c\\cdot\\left(a\\mathit{E}\\left[X\\right] + c\\right) \\\\\n",
    "&=& a^{2}\\mathit{E}\\left[X^{2}\\right] + 2c^{2} + a^{2}\\left(\\mathit{E}\\left[X\\right]\\right)^{2}  + 4ac\\cdot\\mathit{E}\\left[X\\right] - 2a^{2}\\left(\\mathit{E}\\left[X\\right]\\right)^{2}- 2ac\\cdot\\mathit{E}\\left[X\\right] - 2ac\\cdot\\mathit{E}\\left[X\\right] - 2c^{2} \\\\\n",
    "&=& a^{2}\\mathit{E}\\left[X^{2}\\right] - a^{2}\\left(\\mathit{E}\\left[X\\right]\\right)^{2}\\\\\n",
    "&=& a^{2}\\left(\\mathit{E}\\left[X^{2}\\right] - \\left(\\mathit{E}\\left[X\\right]\\right)^{2}\\right)\\\\\n",
    "&=& a^{2}\\textit{Var}\\left[X\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "So adding a constant does nothing for the variance, which is what you would expect since both the values and the expected value of the random variable are shifted by the same amount. Furhtermore we can see that the variance also get's scaled by the square of the factor that is infront of the random variable.\n",
    "\n",
    "Now we almost have everything we need to calculate $\\textit{Var}\\left[\\hat{a}\\right]$. The last thing we will need to know about the variance is when we look at the sum over the values of a random variable (multiplied by a constant $c_{i}$). Without proof this time:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textit{Var}\\left[\\sum_{i=1}^{n}c_{i}\\cdot\\epsilon_{i}\\right] = \\sum_{i=1}^{n}\\textit{Var}\\left[c_{i}\\cdot\\epsilon_{i}\\right] + 2\\cdot \\sum_{i<j}\\sum_{j=1}^{n}c_{i}c_{j}\\textit{Cov}\\left[\\epsilon_{i}\\epsilon_{j}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Equipped with the knowledge about the variances we can finally attempt to calculate $\\textit{Var}\\left[\\hat{a}\\right]$. We will again assume that the values for $X$ are known fixed values so that $\\textit{Var}\\left[\\left(x_{i}-\\bar{x}\\right)\\cdot\\epsilon_{i}\\right] = \\left(x_{i}-\\bar{x}\\right)^{2}\\textit{Var}\\left[\\epsilon_{i}\\right]$ for all $i\\in \\{1,\\dots,n\\}$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textit{Var}\\left[\\hat{a}\\right] \n",
    "&=& \\textit{Var}\\left[\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\right] \\\\\n",
    "&=& \\frac{1}{\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\cdot\\textit{Var}\\left[\\sum_{i=1}^{n}y_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right] \\\\\n",
    "&=& \\frac{1}{\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\cdot\\textit{Var}\\left[\\sum_{i=1}^{n}\\left(ax_{i}+\\epsilon_{i}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)\\right] \\\\\n",
    "&=& \\frac{1}{\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\cdot\\textit{Var}\\left[\\left(\\sum_{i=1}^{n}ax_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right)+\\left(\\sum_{i=1}^{n}\\epsilon_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right)\\right] \\\\\n",
    "&=& \\frac{1}{\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\cdot\\textit{Var}\\left[\\sum_{i=1}^{n}\\epsilon_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right] \\\\\n",
    "&=& \\frac{1}{\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\cdot\\sum_{i=1}^{n}\\textit{Var}\\left[\\epsilon_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right] \\\\\n",
    "&=& \\frac{1}{\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\cdot\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\cdot\\textit{Var}\\left[\\epsilon_{i}\\right] \\\\\n",
    "&=& \\frac{1}{\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\cdot\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\cdot\\sigma_{\\epsilon}^{2} \\\\\n",
    "&=& \\frac{\\sigma_{\\epsilon}^{2}}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "There we go. From here we can easily compute the standard error of the slope $\\textit{SE}\\left[\\hat{a}\\right]$ with:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textit{SE}\\left[\\hat{a}\\right] \n",
    "= \\sqrt{\\textit{Var}\\left[\\hat{a}\\right]}\n",
    "= \\frac{\\sigma_{\\epsilon}}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "With $\\sigma_{\\epsilon}$ as the variance of the error term $\\epsilon$. Since we don't know the the variance of the error term, we need to estimate it with the sample variance $s$:\n",
    "\n",
    "\\begin{equation}\n",
    "s^{2} \n",
    "= \\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\hat{a}x_{i}\\right)^{2}}{n-1} \n",
    "= \\frac{\\textit{RSS}}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "With $\\textit{RSS}$ as the residual sum of squares. For that we can show:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textit{RSS} \n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\hat{a}x_{i}\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(\\left(y_{i}-\\bar{y}\\right)-\\hat{a}\\left(x_{i}-\\bar{x}\\right)\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} + \\hat{a}^{2}\\cdot\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2} - 2\\hat{a}\\cdot\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right) \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} + \\hat{a}\\cdot\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\cdot\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2} - 2\\hat{a}\\cdot\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right) \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} + \\hat{a}\\cdot\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right)\\right) - 2\\hat{a}\\cdot\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right)\\right) \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} - \\hat{a}\\cdot\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right)\\right) \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} - \\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\cdot\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right)\\right) \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} - \\frac{\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\\cdot \\left(1-\\frac{\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\cdot\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}\\right) \\\\\n",
    "&=& \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\\cdot \\left(1-\\rho_{XY}^{2}\\right) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now if we work out the $t$-statistics:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "t \n",
    "&=& \\frac{\\hat{a}-a_{0}}{\\textit{SE}\\left[\\hat{a}\\right]} \\\\\n",
    "&=& \\frac{\\hat{a}\\cdot\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}}{\\sigma_{\\epsilon}} \\\\\n",
    "&\\approx& \\frac{\\hat{a}\\cdot\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}}{s} \\\\\n",
    "&\\approx& \\frac{\\hat{a}\\cdot\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\cdot\\sqrt{n-1}}{\\sqrt{\\textit{RSS}}} \\\\\n",
    "&\\approx& \\frac{\\hat{a}\\cdot\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\cdot\\sqrt{n-1}}{\\sqrt{1-\\rho_{XY}^{2}}\\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}} \\\\\n",
    "&\\approx& \\frac{\\rho_{XY} \\cdot \\frac{\\sigma_{Y}}{\\sigma_{X}}\\cdot\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\cdot\\sqrt{n-1}}{\\sqrt{1-\\rho_{XY}^{2}}\\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}} \\\\\n",
    "&\\approx& \\rho_{XY}\\cdot\\frac{\\sqrt{n-1}}{\\sqrt{1-\\rho_{XY}^{2}}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "And now we can calculate the $t$-value for the given $\\rho_{XY}$ and $n$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 100, we find a t-value of: 0.100\n",
      "For n = 1000, we find a t-value of: 0.316\n",
      "For n = 10000, we find a t-value of: 1.000\n"
     ]
    }
   ],
   "source": [
    "n_list = [10**2, 10**3, 10**4]\n",
    "rho_xy = 0.01\n",
    "\n",
    "for n in n_list:\n",
    "    print(\"For n = {}\".format(n) + \", we find a t-value of: {0:.3f}\".format(rho_xy * (n-1)**0.5 / (1-rho_xy**2)**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $t$-distributions with $n-1$ degrees of freedom can be approximated with a normal distribution for the given value of $n$. Thus we can apply the __$68–95–99.7$ rule__, which means if our __$t$-value is less than two__, then the result is __not significant__ at $95\\,\\%\\,\\textit{CL}\\,$. Hence none of the above are statistical significant at $95\\,\\%\\,\\textit{CL}\\,$. We would need a value of n greater or equal to:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\rho_{XY}\\cdot\\frac{\\sqrt{n-1}}{\\sqrt{1-\\rho_{XY}^{2}}}\\Longleftrightarrow n = \\frac{t^{2}\\left(1-\\rho_{XY}^{2}\\right)}{\\rho_{XY}^{2}} + 1 \\\\\n",
    "n = \\frac{2^{2}\\left(1-0.01^{2}\\right)}{0.01^{2}} + 1 = 39,996\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 40000, we find a t-value of: 2.000\n"
     ]
    }
   ],
   "source": [
    "n = 40000\n",
    "rho_xy = 0.01\n",
    "\n",
    "print(\"For n = {}\".format(n) + \", we find a t-value of: {0:.3f}\".format(rho_xy * (n-1)**0.5 / (1-rho_xy**2)**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though keep in mind that this is an approximation and you should better be using a t-table (or compute the necessary t-value yourself!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
