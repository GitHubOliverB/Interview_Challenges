{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear_Regression_Modelling\n",
    "\n",
    "Consider two data series, $X = \\left(x_{1}, x_{2}, ..., x_{n}\\right)$ and $Y = \\left(y_{1}, y_{2}, ..., y_{n}\\right)$, both with mean zero. We use linear regression (ordinary least squares) to regress $Y$ against $X$ (without ﬁtting any intercept), as in $Y = aX + \\epsilon$ where $\\epsilon$ denotes a series of error terms.\n",
    "\n",
    "Problems:\n",
    "\n",
    "1. Calculate the value of the regression coefﬁcient $a$. If possible, express it in terms of the standard deviations $\\sigma_{X}$ and $\\sigma_{Y}$ and the correlation coefficient $\\rho_{XY}$ between the two data series. You will need to show a complete derivation to score full marks.  \n",
    "\n",
    "2. We scale up both data series by constant factors $s$ and $t$, i.e. $X' = sX$ and $Y' = tY$ , and regress $Y'$ against $X'$ as in $Y' = a'X' + \\epsilon$. How does the new regression coefﬁcient $a'$ relate to the original coefﬁcient $a$? And what about the new correlation $\\rho_{X'Y'}$ vs. the original correlation $\\rho_{XY}$ ? Note that the new $\\epsilon$ is not necessarily the same as the original one, it merely denotes another series of error terms.  \n",
    "\n",
    "3. We now do the ‘inverse’ regression of $X$ against $Y$ , resulting in $X = bY + \\epsilon$. How is the slope $b$ of the ‘inverse’ regression related to the slope a of the original regression?   \n",
    "\n",
    "4. Suppose that $\\rho_{XY} = 0.01$. Is the resulting value of $a$ statistically signiﬁcantly different from $0$ at the $95\\,\\%$ level if:   \n",
    "    i. $n = 10^{2}$  \n",
    "    ii. $n = 10^{3}$  \n",
    "    iii. $n = 10^{4}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:\n",
    "\n",
    "The idea of linear regression is to use a function $f^{*}\\left(X\\right)$ that is linear in a set of parameters $a_{i}\\in A$ to predict $Y$ as close as possible. The function $f^{*}\\left(X\\right)$ is derived by chosing the parameters $a_{i}\\in A$ for a function $f\\left(A, X\\right)$ so that this is achieved.\n",
    "\n",
    "In our case we work with $f\\left(a, X\\right) = a\\cdot X$ so that $Y = a\\cdot X + \\epsilon$. \n",
    "\n",
    "When we use ordinary least square to regress $Y$ to $X$, what we want to do is to minimize is the squared difference between our observed values $Y$ and the prediction from our function $f\\left(a, X\\right)$. We denote that as our loss function $L$, given by:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\sum_{i=1}^{n} \\left(y_{i}-a\\cdot x_{i}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "By minimizing $L$, we can find the set of parameters for which $f\\left(A, X\\right)$ becomes $f^{*}\\left(X\\right)$, so let's do that now.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial a} &=& \\frac{\\partial}{\\partial a}\\sum_{i=1}^{n} \\left(y_{i}-a\\cdot x_{i}\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}\\frac{\\partial}{\\partial a} \\left(y_{i}-a\\cdot x_{i}\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}2\\cdot\\left(y_{i}-a\\cdot x_{i}\\right)\\cdot\\left(-x_{i}\\right) \\\\\n",
    "&=& -2\\cdot\\sum_{i=1}^{n}\\left(y_{i}\\cdot x_{i}-a\\cdot x_{i}^{2}\\right) \\\\\n",
    "&=& -2\\cdot\\left(\\sum_{i=1}^{n}y_{i}\\cdot x_{i}\\right) + 2\\cdot\\left(\\sum_{i=1}^{n} a\\cdot x_{i}^{2}\\right) \\\\\n",
    "&=& -2\\cdot\\left(\\sum_{i=1}^{n}y_{i}\\cdot x_{i}\\right) + 2a\\cdot\\left(\\sum_{i=1}^{n} x_{i}^{2}\\right) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "From setting $\\frac{\\partial L}{\\partial a}=0$, it follows that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a\\cdot\\sum_{i=1}^{n} x_{i}^{2} &=& \\sum_{i=1}^{n}y_{i}\\cdot x_{i}  \\\\\n",
    "a &=& \\frac{\\sum_{i=1}^{n}y_{i}\\cdot x_{i}}{\\sum_{i=1}^{n} x_{i}^{2}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now we have found an expression for $a$ in terms of sums over $x_{i}$ and $y_{i}$. However we want to express it in terms of the standard deviations $\\sigma_{X}$ and $\\sigma_{Y}$ and the correlation $\\rho_{XY}$. So let's work out those and see if we can substitute them in our result.\n",
    "\n",
    "The standard deviations $\\sigma_{k}$ for a sample with $k = X, Y$ and $\\bar{k}=\\frac{1}{n}\\sum_{i=1}^{n}k_i$ as the mean value of $k$ is given by:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{k}^{2} = \\frac{\\sum_{i=1}^{n}\\left(k_{i}-\\bar{k}\\right)^{2}}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "Now since the mean values of $X$ and $Y$ are both zero, we can simplify the standard deviations by substituting $\\bar{k}=0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{k}^{2} = \\frac{\\sum_{i=1}^{n}k_{i}^{2}}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "The correlation coefficient $\\rho_{XY}$ for a sample is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{XY} = \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "Using our mean values of zero, we simply get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{XY} = \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}\\sqrt{\\sum_{i=1}^{n}y_{i}^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "We can expand our equation for $\\rho_{XY}$ by multiplying with $\\frac{1}{\\frac{n-1}{n-1}}$ and get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\rho_{XY} &=& \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sqrt{\\frac{\\sum_{i=1}^{n}x_{i}^{2}}{n-1}}\\sqrt{\\frac{\\sum_{i=1}^{n}y_{i}^{2}}{n-1}}}\\cdot \\frac{1}{n-1} \\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\sigma_{Y}}\\cdot \\frac{1}{n-1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We recognize that the sums in $sigma_{X}$ and in $\\rho_{XY}$ appear in $a$, so let's expand $a$ so that we substitute in both:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a &=& \\frac{\\sum_{i=1}^{n}y_{i}\\cdot x_{i}}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\sum_{i=1}^{n}y_{i}\\cdot x_{i} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}\\sigma_{Y}}\\frac{n-1}{n-1} \\cdot \\frac{1}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\left(\\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\sigma_{Y}}\\cdot \\frac{1}{n-1}\\right) \\cdot \\frac{\\sigma_{X}\\sigma_{Y}\\cdot\\left(n-1\\right)}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\frac{\\sum_{i=1}^{n} x_{i}^{2}}{n-1}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}^{2}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{Y}}{\\sigma_{X}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "So we find that $a$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\rho_{XY} \\cdot \\frac{\\sigma_{Y}}{\\sigma_{X}}\n",
    "\\end{equation}\n",
    "\n",
    "I want to test this, so first let's grab a useful expression for $\\rho_{XY}$. Using the covariance $\\textit{cov}\\left(X,Y\\right)$ of $X$ and $Y$ given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textit{cov}\\left(X,Y\\right) = \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(y_{i}-\\bar{y}\\right)}{n-1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We can rewrite the correlation coefficient as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\rho_{XY} = \\frac{\\textit{cov}\\left(X,Y\\right)}{\\sigma_{X}\\sigma_{Y}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Thus $a$ can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a &=& \\frac{\\textit{cov}\\left(X,Y\\right)}{\\sigma_{X}^{2}} \\\\\n",
    "&=& \\frac{\\textit{cov}\\left(X,Y\\right)}{\\textit{var}\\left(X\\right)}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Y: 0.00\n",
      "Mean of X: 0.01\n",
      "Due to statistical fluctuations the mean value of X is not exactly 0.\n",
      "\n",
      "Our guess for the slope a is: 0.989\n",
      "\n",
      "Running the linear regression with a forced intercept of 0, we get:\n",
      "\n",
      "Slope a: 0.989\n",
      "Intercept: 0.0\n",
      "\n",
      " The difference between our guess and the found slope is: 0.00000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mu_X = range(-10,11,1)\n",
    "sigma_X = 0.5\n",
    "measurements = 10\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for mu in mu_X:\n",
    "    Y.extend([mu]*measurements)\n",
    "    X.extend(np.random.normal(mu, sigma_X, measurements))\n",
    "\n",
    "print(\"Mean of Y: {0:.2f}\".format(np.mean(Y)))\n",
    "print(\"Mean of X: {0:.2f}\".format(np.mean(X)))\n",
    "if np.mean(X) != 0:\n",
    "    print(\"Due to statistical fluctuations the mean value of X is not exactly 0.\\n\")\n",
    "    \n",
    "cov_XY_matrix = np.cov(X,Y)\n",
    "cov_XY = cov_XY_matrix[0,1]\n",
    "var_X = cov_XY_matrix[0,0]\n",
    "\n",
    "a = cov_XY / var_X\n",
    "\n",
    "print(\"Our guess for the slope a is: {0:.3f}\\n\".format(a))\n",
    "\n",
    "# Do linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_reg = np.array(X).reshape((-1, 1))\n",
    "\n",
    "reg = LinearRegression(fit_intercept=0).fit(X_reg, Y)\n",
    "\n",
    "print(\"Running the linear regression with a forced intercept of 0, we get:\\n\")\n",
    "print(\"Slope a: {0:.3f}\".format(reg.coef_[0]))\n",
    "print(\"Intercept: {}\".format(reg.intercept_))\n",
    "\n",
    "print(\"\\n The difference between our guess and the found slope is: {0:.5f}\".format(abs(reg.coef_[0]-a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see our result seems to be pretty good. Since $\\bar{x}$ is not exactly $0$, we have a small difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
