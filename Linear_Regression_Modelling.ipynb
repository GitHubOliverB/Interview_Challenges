{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear_Regression_Modelling\n",
    "\n",
    "Consider two data series, $X = \\left(x_{1}, x_{2}, ..., x_{n}\\right)$ and $Y = \\left(y_{1}, y_{2}, ..., y_{n}\\right)$, both with mean zero. We use linear regression (ordinary least squares) to regress $Y$ against $X$ (without ﬁtting any intercept), as in $Y = aX + \\epsilon$ where $\\epsilon$ denotes a series of error terms.\n",
    "\n",
    "Problems:\n",
    "\n",
    "1. Calculate the value of the regression coefﬁcient $a$. If possible, express it in terms of the standard deviations $\\sigma_{X}$ and $\\sigma_{Y}$ and the correlation coefficient $\\rho_{XY}$ between the two data series. You will need to show a complete derivation to score full marks.  \n",
    "\n",
    "2. We scale up both data series by constant factors $s$ and $t$, i.e. $X' = sX$ and $Y' = tY$ , and regress $Y'$ against $X'$ as in $Y' = a'X' + \\epsilon$. How does the new regression coefﬁcient $a'$ relate to the original coefﬁcient $a$? And what about the new correlation $\\rho_{X'Y'}$ vs. the original correlation $\\rho_{XY}$ ? Note that the new $\\epsilon$ is not necessarily the same as the original one, it merely denotes another series of error terms.  \n",
    "\n",
    "3. We now do the ‘inverse’ regression of $X$ against $Y$ , resulting in $X = bY + \\epsilon$. How is the slope $b$ of the ‘inverse’ regression related to the slope $a$ of the original regression?   \n",
    "\n",
    "4. Suppose that $\\rho_{XY} = 0.01$. Is the resulting value of $a$ statistically signiﬁcantly different from $0$ at the $95\\,\\%$ level if:   \n",
    "    i. $n = 10^{2}$  \n",
    "    ii. $n = 10^{3}$  \n",
    "    iii. $n = 10^{4}$  \n",
    "\n",
    "In order to make life a bit easier, I'm going to assume that $\\epsilon \\sim \\mathcal{N}\\left(0,\\sigma_{\\epsilon}^{2}\\right)$ and $\\textit{Cov}\\left(\\epsilon_{i}, \\epsilon_{j}\\right) = \\sigma_{\\epsilon}^{2} \\cdot \\delta_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:\n",
    "\n",
    "The idea of linear regression is to use a function $f^{*}\\left(X\\right)$ that is linear in a set of parameters $a_{i}\\in A$ to predict $Y$ as close as possible. The function $f^{*}\\left(X\\right)$ is derived by chosing the parameters $a_{i}\\in A$ for a function $f\\left(A, X\\right)$ so that this is achieved.\n",
    "\n",
    "In our case we work with $f\\left(a, X\\right) = a\\cdot X$ so that $Y = a\\cdot X + \\epsilon$. \n",
    "\n",
    "When we use ordinary least square to regress $Y$ to $X$, what we want to do is to minimize is the squared difference between our observed values $Y$ and the prediction from our function $f\\left(a, X\\right)$. We denote that as our loss function $L$, given by:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\sum_{i=1}^{n} \\left(y_{i}-a\\cdot x_{i}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "By minimizing $L$, we can find the set of parameters for which $f\\left(A, X\\right)$ becomes $f^{*}\\left(X\\right)$, so let's do that now.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial a} &=& \\frac{\\partial}{\\partial a}\\sum_{i=1}^{n} \\left(y_{i}-a\\cdot x_{i}\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}\\frac{\\partial}{\\partial a} \\left(y_{i}-a\\cdot x_{i}\\right)^{2} \\\\\n",
    "&=& \\sum_{i=1}^{n}2\\cdot\\left(y_{i}-a\\cdot x_{i}\\right)\\cdot\\left(-x_{i}\\right) \\\\\n",
    "&=& -2\\cdot\\sum_{i=1}^{n}\\left(y_{i}\\cdot x_{i}-a\\cdot x_{i}^{2}\\right) \\\\\n",
    "&=& -2\\cdot\\left(\\sum_{i=1}^{n}y_{i}\\cdot x_{i}\\right) + 2\\cdot\\left(\\sum_{i=1}^{n} a\\cdot x_{i}^{2}\\right) \\\\\n",
    "&=& -2\\cdot\\left(\\sum_{i=1}^{n}y_{i}\\cdot x_{i}\\right) + 2a\\cdot\\left(\\sum_{i=1}^{n} x_{i}^{2}\\right) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "From setting $\\frac{\\partial L}{\\partial a}=0$, it follows that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a\\cdot\\sum_{i=1}^{n} x_{i}^{2} &=& \\sum_{i=1}^{n}y_{i}\\cdot x_{i}  \\\\\n",
    "a &=& \\frac{\\sum_{i=1}^{n}y_{i}\\cdot x_{i}}{\\sum_{i=1}^{n} x_{i}^{2}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now we have found an expression for $a$ in terms of sums over $x_{i}$ and $y_{i}$. However we want to express it in terms of the standard deviations $\\sigma_{X}$ and $\\sigma_{Y}$ and the correlation $\\rho_{XY}$. So let's work out those and see if we can substitute them in our result.\n",
    "\n",
    "The standard deviations $\\sigma_{k}$ for a sample with $k = X, Y$ and $\\bar{k}=\\frac{1}{n}\\sum_{i=1}^{n}k_i$ as the mean value of $k$ is given by:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{k}^{2} = \\frac{\\sum_{i=1}^{n}\\left(k_{i}-\\bar{k}\\right)^{2}}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "Now since the mean values of $X$ and $Y$ are both zero, we can simplify the standard deviations by substituting $\\bar{k}=0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{k}^{2} = \\frac{\\sum_{i=1}^{n}k_{i}^{2}}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "The correlation coefficient $\\rho_{XY}$ for a sample is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{XY} = \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "Using our mean values of zero, we simply get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{XY} = \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}\\sqrt{\\sum_{i=1}^{n}y_{i}^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "We can expand our equation for $\\rho_{XY}$ by multiplying with $\\frac{1}{\\frac{n-1}{n-1}}$ and get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\rho_{XY} &=& \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sqrt{\\frac{\\sum_{i=1}^{n}x_{i}^{2}}{n-1}}\\sqrt{\\frac{\\sum_{i=1}^{n}y_{i}^{2}}{n-1}}}\\cdot \\frac{1}{n-1} \\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\sigma_{Y}}\\cdot \\frac{1}{n-1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We recognize that the sums in $sigma_{X}$ and in $\\rho_{XY}$ appear in $a$, so let's expand $a$ so that we substitute in both:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a &=& \\frac{\\sum_{i=1}^{n}y_{i}\\cdot x_{i}}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\sum_{i=1}^{n}y_{i}\\cdot x_{i} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}\\sigma_{Y}}\\frac{n-1}{n-1} \\cdot \\frac{1}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\left(\\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\sigma_{Y}}\\cdot \\frac{1}{n-1}\\right) \\cdot \\frac{\\sigma_{X}\\sigma_{Y}\\cdot\\left(n-1\\right)}{\\sum_{i=1}^{n} x_{i}^{2}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\frac{\\sum_{i=1}^{n} x_{i}^{2}}{n-1}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}^{2}} \\\\\n",
    "&=& \\rho_{XY} \\cdot \\frac{\\sigma_{Y}}{\\sigma_{X}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "So we find that $a$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\rho_{XY} \\cdot \\frac{\\sigma_{Y}}{\\sigma_{X}}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore our function $f^{*}\\left(X\\right)$ is given by:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f^{*}\\left(X\\right) = \\frac{\\rho_{XY}\\cdot\\sigma_{Y}}{\\sigma_{X}} \\cdot X\n",
    "\\end{equation}\n",
    "\n",
    "I want to test this, so first let's grab a useful expression for $\\rho_{XY}$. Using the covariance $\\textit{cov}\\left(X,Y\\right)$ of $X$ and $Y$ given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\textit{cov}\\left(X,Y\\right) = \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(y_{i}-\\bar{y}\\right)}{n-1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We can rewrite the correlation coefficient as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\rho_{XY} = \\frac{\\textit{cov}\\left(X,Y\\right)}{\\sigma_{X}\\sigma_{Y}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Thus $a$ can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a &=& \\frac{\\textit{cov}\\left(X,Y\\right)}{\\sigma_{X}^{2}} \\\\\n",
    "&=& \\frac{\\textit{cov}\\left(X,Y\\right)}{\\textit{var}\\left(X\\right)}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Y: 0.00\n",
      "Mean of X: 0.05\n",
      "Due to statistical fluctuations the mean value of X is not exactly 0.\n",
      "\n",
      "Our guess for the slope a is: 0.990\n",
      "\n",
      "Running the linear regression with a forced intercept of 0, we get:\n",
      "\n",
      "Slope a: 0.990\n",
      "Intercept: 0.0\n",
      "\n",
      " The difference between our guess and the found slope is: 0.00006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mu_X = range(-10,11,1)\n",
    "sigma_X = 0.5\n",
    "measurements = 10\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for mu in mu_X:\n",
    "    Y.extend([mu]*measurements)\n",
    "    X.extend(np.random.normal(mu, sigma_X, measurements))\n",
    "\n",
    "print(\"Mean of Y: {0:.2f}\".format(np.mean(Y)))\n",
    "print(\"Mean of X: {0:.2f}\".format(np.mean(X)))\n",
    "if np.mean(X) != 0:\n",
    "    print(\"Due to statistical fluctuations the mean value of X is not exactly 0.\\n\")\n",
    "    \n",
    "cov_XY_matrix = np.cov(X,Y)\n",
    "cov_XY = cov_XY_matrix[0,1]\n",
    "var_X = cov_XY_matrix[0,0]\n",
    "var_Y = cov_XY_matrix[1,1]\n",
    "\n",
    "a = cov_XY / var_X\n",
    "\n",
    "print(\"Our guess for the slope a is: {0:.3f}\\n\".format(a))\n",
    "\n",
    "# Do linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_reg = np.array(X).reshape((-1, 1))\n",
    "\n",
    "reg = LinearRegression(fit_intercept=0).fit(X_reg, Y)\n",
    "\n",
    "print(\"Running the linear regression with a forced intercept of 0, we get:\\n\")\n",
    "print(\"Slope a: {0:.3f}\".format(reg.coef_[0]))\n",
    "print(\"Intercept: {}\".format(reg.intercept_))\n",
    "\n",
    "print(\"\\n The difference between our guess and the found slope is: {0:.5f}\".format(abs(reg.coef_[0]-a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see our result seems to be pretty good. Since $\\bar{x}$ is not exactly $0$, we have a small difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:\n",
    "\n",
    "So let's see what scaling both $X$ and $Y$ does to everything that we have calculated so far. In order to do that we simply have to follow:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "X &\\longrightarrow& X' = s\\cdot X \\\\\n",
    "Y &\\longrightarrow& Y' = t\\cdot Y\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We can start by the mean and find that is scaled by the specific factor, but since the old mean was zero, it remains unchanged:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\bar{x}' &=& \\frac{1}{n}\\cdot\\sum_{i=1}^{n} x_{i}' &=& \\frac{1}{n}\\cdot\\sum_{i=1}^{n}s\\cdot x_{i} &=& \\frac{s}{n}\\bar{x} &=& 0\\\\\n",
    "\\bar{y}' &=&  \\frac{1}{n}\\cdot\\sum_{i=1}^{n} y_{i}' &=& \\frac{1}{n}\\cdot\\sum_{i=1}^{n}t\\cdot y_{i} &=& \\frac{t}{n}\\bar{y} &=& 0\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Knowing $\\bar{x}'$ and $\\bar{y}'$ we can work out $\\sigma_{X'}$ and $\\sigma_{Y'}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sigma_{X'}^{2} &=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}'-\\bar{x}'\\right)^{2}}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}s^{2}x_{i}^{2}}{n-1} \n",
    "= s^{2}\\cdot\\frac{\\sum_{i=1}^{n}x_{i}^{2}}{n-1} \n",
    "= s^{2}\\sigma_{X}^{2} \\\\\n",
    "\\\\\n",
    "\\sigma_{Y'}^{2} &=& \\frac{\\sum_{i=1}^{n}\\left(y_{i}'-\\bar{y}'\\right)^{2}}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}t^{2}y_{i}^{2}}{n-1} \n",
    "= t^{2}\\cdot\\frac{\\sum_{i=1}^{n}y_{i}^{2}}{n-1} \n",
    "= t^{2}\\sigma_{Y}^{2} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "And from there we can look at $\\rho_{X'Y'}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{X'Y'} = \\frac{\\sum_{i=1}^{n}x_{i}'\\cdot y_{i}'}{\\sigma_{X'}\\sigma_{Y'}}\\cdot \\frac{1}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}sx_{i} \\cdot t y_{i}}{s\\sigma_{X}\\cdot t\\sigma_{Y}}\\cdot \\frac{1}{n-1} \n",
    "= \\frac{st\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{st\\cdot\\sigma_{X}\\cdot\\sigma_{Y}}\\cdot \\frac{1}{n-1} \n",
    "= \\frac{\\sum_{i=1}^{n}x_{i}\\cdot y_{i}}{\\sigma_{X}\\cdot\\sigma_{Y}}\\cdot \\frac{1}{n-1} \n",
    "= \\rho_{XY}\n",
    "\\end{equation}\n",
    "\n",
    "As we can see $\\rho_{X'Y'}$ is unchanged just like the mean values of $X$ and $Y$. Now we have everything we need to calculate the slope $a'$:\n",
    "\n",
    "\\begin{equation}\n",
    "a' = \\frac{\\rho_{X'Y'}\\cdot\\sigma_{Y'}}{\\sigma_{X'}}\n",
    "= \\frac{\\rho_{XY}\\cdot t\\sigma_{Y}}{s\\sigma_{X}}\n",
    "= \\frac{t}{s}\\cdot \\frac{\\rho_{XY}\\cdot\\sigma_{Y}}{\\sigma_{X}}\n",
    "= \\frac{t}{s}\\cdot a\n",
    "\\end{equation}\n",
    "\n",
    "So finally, we can see that $a'$ is just $a$ scaled by the ratio $\\frac{t}{s}$. That makes perfect scence, since we also could have started with $Y' = a'X' + \\epsilon$ and work out $a'$ from there in the following fashion:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Y' = a'X' + \\epsilon \\\\\n",
    "\\\\\n",
    "tY = a'sX+ \\epsilon \\\\\n",
    "\\\\\n",
    "Y = \\frac{s}{t}\\cdot a' \\cdot X + \\frac{\\epsilon}{t}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "And recognize that :\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\frac{s}{t}\\cdot a'\n",
    "\\end{equation}\n",
    "\n",
    "Furhtermore, we can also think through a few cases. If we left $Y$ unscaled, so $t=1$ and we scale up $X$ by a factor $s>1$, then $a'$ must be smaller than $a$ to compensate the upscaling of $X$ for $Y$ to remain unchanged. On the other hand, if we scale up $Y$ by a factor $t>1$ and we left $X$ unchanged, then $a'$ must be bigger than $a$ to compensate the upscaling of $Y$. So $a'$ must depend on both scaling factors.\n",
    "\n",
    "Now let's check that again with the same piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Y_scale: -0.00\n",
      "Mean of X_scale: 0.35\n",
      "Due to statistical fluctuations the mean value of X_scale is not exactly 0.\n",
      "\n",
      "Our guess for the slope a_scale is: 0.027\n",
      "\n",
      "Running the linear regression with a forced intercept of 0, we get:\n",
      "\n",
      "Slope a_scale: 0.027\n",
      "Intercept_scale: 0.0\n",
      "\n",
      " The difference between our guess and the found slope is: 0.00000\n"
     ]
    }
   ],
   "source": [
    "s = 7.4\n",
    "t = 1/5\n",
    "\n",
    "X_scale = [s*x for x in X]\n",
    "Y_scale = [t*y for y in Y]\n",
    "\n",
    "print(\"Mean of Y_scale: {0:.2f}\".format(np.mean(Y_scale)))\n",
    "print(\"Mean of X_scale: {0:.2f}\".format(np.mean(X_scale)))\n",
    "if np.mean(X_scale) != 0:\n",
    "    print(\"Due to statistical fluctuations the mean value of X_scale is not exactly 0.\\n\")\n",
    "    \n",
    "a_scale = a * t/s\n",
    "\n",
    "print(\"Our guess for the slope a_scale is: {0:.3f}\\n\".format(a_scale))\n",
    "\n",
    "# Do linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_scale_reg = np.array(X_scale).reshape((-1, 1))\n",
    "\n",
    "reg_scale = LinearRegression(fit_intercept=0).fit(X_scale_reg, Y_scale)\n",
    "\n",
    "print(\"Running the linear regression with a forced intercept of 0, we get:\\n\")\n",
    "print(\"Slope a_scale: {0:.3f}\".format(reg_scale.coef_[0]))\n",
    "print(\"Intercept_scale: {}\".format(reg_scale.intercept_))\n",
    "\n",
    "print(\"\\n The difference between our guess and the found slope is: {0:.5f}\".format(abs(reg_scale.coef_[0]-a_scale)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:\n",
    "\n",
    "Since we didn't change the structure of our problem, we can get the result by taking the old result and act with the following transformation on it:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Y &\\longrightarrow& X \\\\\n",
    "X &\\longrightarrow& Y \\\\\n",
    "a &\\longrightarrow& b \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Hence:\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\rho_{YX}\\cdot\\sigma_{X}}{\\sigma_{Y}} \n",
    "= \\frac{\\rho_{XY}\\sigma_{X}}{\\sigma_{Y}}\n",
    "= \\frac{\\rho_{XY}\\sigma_{X}}{\\sigma_{Y}}\\cdot\\frac{\\sigma_{X}}{\\sigma_{X}}\\cdot\\frac{\\sigma_{Y}}{\\sigma_{Y}}\n",
    "= \\frac{\\rho_{XY}\\sigma_{Y}}{\\sigma_{X}}\\cdot\\frac{\\sigma_{X}^{2}}{\\sigma_{Y}^{2}}\n",
    "= a\\cdot \\frac{\\sigma_{X}^{2}}{\\sigma_{Y}^{2}}\n",
    "= a\\cdot \\frac{\\textit{var}\\left(X\\right)}{\\textit{var}\\left(Y\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "So let us test that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our guess for the slope b is: 1.003\n",
      "\n",
      "Running the linear regression with a forced intercept of 0, we get:\n",
      "\n",
      "Slope b: 1.003\n",
      "Intercept inverse: 0.0\n",
      "\n",
      " The difference between our guess and the found slope is: 0.00000\n"
     ]
    }
   ],
   "source": [
    "b = a * var_X / var_Y\n",
    "\n",
    "print(\"Our guess for the slope b is: {0:.3f}\\n\".format(b))\n",
    "\n",
    "# Do linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "Y_reg = np.array(Y).reshape((-1, 1))\n",
    "\n",
    "reg_inverse = LinearRegression(fit_intercept=0).fit(Y_reg, X)\n",
    "\n",
    "print(\"Running the linear regression with a forced intercept of 0, we get:\\n\")\n",
    "print(\"Slope b: {0:.3f}\".format(reg_inverse.coef_[0]))\n",
    "print(\"Intercept inverse: {}\".format(reg_inverse.intercept_))\n",
    "\n",
    "print(\"\\n The difference between our guess and the found slope is: {0:.5f}\".format(abs(reg_inverse.coef_[0]-b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, let's move on to the final problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4:\n",
    "\n",
    "Now we want to check two hypotheses, the null-hypothesis $H_{0}$ stating that the slope $a$ is zero and the alternative hypothesis $H_{a}$ stating that the slope $a$ is not zero.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "H_{0}: a = 0 \\\\\n",
    "H_{a}: a \\neq 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now I normally would calculate the test statistic $t$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\hat{a}-a_{0}}{\\textit{SE}\\left[\\hat{a}\\right]}\n",
    "\\end{equation}\n",
    "\n",
    "With $\\hat{a}$ as our estimated value for the slope $a$, $a_0 = 0$ the value of for the slope a in the case of our null-hypothesis and $\\textit{SE}\\left[\\hat{a}\\right]$ as the standard error for $\\hat{a}$. Once we have the t-statistic, we can calculate the values for a given $n$ and compare that to a table for the $95\\,\\%$ significant level and see if our result is significant.\n",
    "\n",
    "Now, I don't know a simple formula to calculate that by heart, so let's derive it (it will be long).\n",
    "\n",
    "Let's start by finding the expected value for $\\hat{a}$. For that we will our result for $\\hat{a}$ from problem 1:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{a} = \\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "Given that both $\\sum_{i=1}^{n}\\left(k_{i}-\\bar{k}\\right) = 0$ for $k = x, y$, we can expand both sums and get rid of the term that is multiplied by $\\bar{k}$, since we can just pull that infront of the sum. Hence:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{a} = \\frac{\\sum_{i=1}^{n}y_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "Furhtermore we will need a neat little propertie of the expected value called the linearity of expectations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathit{E}\\left[aX+bY\\right] = \\frac{1}{n}\\cdot\\sum_{i=1}^{n}\\left(aX+bY\\right)\n",
    "= \\frac{1}{n}\\cdot\\left(\\sum_{i=1}^{n}aX\\right) + \\frac{1}{n}\\cdot\\left(\\sum_{i=1}^{n}bY\\right)\n",
    "= \\frac{a}{n}\\cdot\\left(\\sum_{i=1}^{n}X\\right) + \\frac{b}{n}\\cdot\\left(\\sum_{i=1}^{n}Y\\right)\n",
    "= a\\cdot \\mathit{E}\\left[X\\right] + b \\cdot \\mathit{E}\\left[Y\\right]\n",
    "\\end{equation}\n",
    "\n",
    "On top of that we will assume $X$ to be known fixed values so that $\\mathit{E}\\left[x_{i}\\right] = x_{i}$, $\\forall i \\in \\{1, \\dots, n\\}$ and that the error terms are normally distributed around the mean value of $0$ so that $\\mathit{E}\\left[\\epsilon_{i}\\right] = 0$, $\\forall i \\in \\{1, \\dots, n\\}$.\n",
    "\n",
    "Now let's begin to derive the expected value of $\\hat{a}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathit{E}\\left[\\hat{a}\\right] &=& \\mathit{E}\\left[\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\right]\\\\\n",
    "&=& \\frac{1}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}\\cdot\\mathit{E}\\left[\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]\\\\\n",
    "&=& \\frac{\\mathit{E}\\left[\\sum_{i=1}^{n}y_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\mathit{E}\\left[y_{i}\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\mathit{E}\\left[\\left(a\\cdot x_{i} + \\epsilon_{i}\\right)\\cdot\\left(x_{i}-\\bar{x}\\right)\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\mathit{E}\\left[a\\cdot x_{i} + \\epsilon_{i}\\right]}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot\\left(a\\cdot x_{i}+\\mathit{E}\\left[\\epsilon_{i}\\right]\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot a\\cdot x_{i}}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\cdot x_{i}}\\\\\n",
    "&=& a \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Great. As we can see we have an unbiased estimater for the slope $a$.\n",
    "\n",
    "Now let's have a look at the variance of $\\hat{a}$. But again let's quickly introduce some properties that we will need. Also the assumption about our fixed known values of $X$ still stands. We will start of with the definition of the variance and show that there is no linearity for the variance and that the variance of a constant vanishes. Given the definition of the variance\n",
    "\n",
    "\\begin{equation}\n",
    "\\textit{Var}\\left[X\\right] = \\mathit{E}\\left[\\left(X-\\mathit{E}\\left[X\\right]\\right)^{2}\\right]\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "we can work out that for a constant $a$ and $c$ and a random variable $X$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textit{Var}\\left[aX+c\\right] = \\mathit{E}\\left[\\left(aX+c-\\mathit{E}\\left[aX+c\\right]\\right)^{2}\\right]\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The standard error of the slope $a$ is computed by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textit{SE}\\left[\\hat{a}\\right] = \\frac{\\sigma_{\\epsilon}}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "With $\\sigma_{\\epsilon}$ as the variance of the error term $\\epsilon$. Since we don't know the the variance of the error term, we need to estimate it with the sample variance $\\hat{\\sigma}_{\\epsilon}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}_{\\epsilon}^{2} = \\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\hat{a}x_{i}\\right)^{2}}{n-2}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
